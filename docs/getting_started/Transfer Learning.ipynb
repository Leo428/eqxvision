{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "vgg_finetune.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "machine_shape": "hm",
   "authorship_tag": "ABX9TyPL3ALvz8zV7SLiftFdu/TK",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/paganpasta/eqxvision/blob/documentation%2Fgetting_started%2Fself-attention/docs/getting_started/vgg_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Transfer Learning\n",
    "\n",
    "Here we will try to hit two birds with one stone\n",
    "  - Prepare a tutorial on transfer learning.\n",
    "  - Verify the goodness of `VGG` models\n",
    "\n",
    "To those of you not aware, pretrained VGGs in `eqxvision` perform poorly in comparison to `torchvision` counterparts. The main reason is due to differences in implementation of Equinox's `adaptive average pooling` ( which I implemented :sad_face: )\n",
    "\n",
    "The flow of this tutorial will be as:\n",
    "\n",
    "  - Preparing train/val datasets\n",
    "  - Preparing model\n",
    "  - Setting up forward and loss computation methods\n",
    "  - Initialising the optimizer\n",
    "  - Model Training\n",
    "  - Verifying the integrity of weights\n",
    "\n",
    "  ---"
   ],
   "metadata": {
    "id": "Y9uEFbb8X3I9",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing Dependencies"
   ],
   "metadata": {
    "id": "Pljy9RHFaB-A",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MwKhbfH6BEOC",
    "outputId": "35cf3d2b-4b64-4e12-fcb6-b1a119e4d36f",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001B[K     |████████████████████████████████| 145 kB 24.1 MB/s \n",
      "\u001B[K     |████████████████████████████████| 66 kB 4.8 MB/s \n",
      "\u001B[K     |████████████████████████████████| 76 kB 5.5 MB/s \n",
      "\u001B[?25h"
     ]
    }
   ],
   "source": [
    "!pip install eqxvision optax --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Basic Imports"
   ],
   "metadata": {
    "id": "eJPizb7uYr0F",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import functools as ft\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import optax\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "import eqxvision as eqv"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hyper-parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "LR = 0.001\n",
    "EPOCHS = 5"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Dataset & Dataloaders"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.RandomResizedCrop(224),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "val_transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    ]\n",
    ")\n",
    "train_dataset = datasets.STL10(\n",
    "    root=\"/tmp\", split=\"train\", transform=train_transform, download=True\n",
    ")\n",
    "val_dataset = datasets.STL10(\n",
    "    root=\"/tmp\", split=\"test\", transform=val_transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, num_workers=2, batch_size=BATCH_SIZE)\n",
    "val_loader = DataLoader(dataset=val_dataset, num_workers=2, batch_size=BATCH_SIZE)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model Prep.\n",
    "\n",
    "We need to perform two steps after initialising the model.\n",
    "\n",
    "1. Replace the final classification layer to suit the `STL-10` dataset.\n",
    "2. Freeze the parameters for all layers except the classification layer."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = eqv.models.vgg11(pretrained=True)\n",
    "\n",
    "# Replacing the last layer for STL-10\n",
    "model = eqx.tree_at(\n",
    "    lambda m: m.classifier,\n",
    "    model,\n",
    "    (eqx.nn.Linear(512 * 7 * 7, 10, key=jrandom.PRNGKey(0))),\n",
    ")\n",
    "\n",
    "# Freezing the model except for the last layer\n",
    "filter_spec = jax.tree_util.tree_map(lambda _: False, model)\n",
    "filter_spec = eqx.tree_at(\n",
    "    lambda tree: (tree.classifier.weight, tree.classifier.bias),\n",
    "    filter_spec,\n",
    "    replace=(True, True),\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Utility Methods\n",
    "\n",
    "The `filter_spec` decides the params w.r.t to which the gradient is computed.\n",
    "Here, we will be computing gradient w.r.t to only the `classifier` module.\n",
    "\n",
    "Check [here](https://docs.kidger.site/equinox/examples/frozen_layer/) for more details."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@ft.partial(eqx.filter_value_and_grad, arg=filter_spec)\n",
    "def compute_loss(model, x, y, keys):\n",
    "    logits = jax.vmap(model, axis_name=(\"batch\"))(x, key=keys)\n",
    "    one_hot_actual = jax.nn.one_hot(y, num_classes=10)\n",
    "    return optax.softmax_cross_entropy(logits, one_hot_actual).mean()\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def make_step(model, x, y, keys, optimizer, opt_state):\n",
    "    loss, grads = compute_loss(model, x, y, keys)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    model = eqx.apply_updates(model, updates)\n",
    "    return loss, model, opt_state\n",
    "\n",
    "\n",
    "def accuracy(model, loader):\n",
    "    correct = 0.0\n",
    "    total = 0.0\n",
    "    for images, labels in loader:\n",
    "        keys = jrandom.split(jrandom.PRNGKey(0), images.shape[0])\n",
    "        output = jax.vmap(model, axis_name=\"batch\")(\n",
    "            jnp.asarray(images.numpy()), key=keys\n",
    "        )\n",
    "        pred = jnp.argmax(output, axis=1)\n",
    "\n",
    "        correct += jnp.sum(pred == labels.numpy())\n",
    "        total += images.shape[0]\n",
    "    return correct / total"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optimizer & Scheduler\n",
    "\n",
    "The important bit to remember is wrapping the model in `eqx.filter` before passing it on to the optimizer. This step will `fail` if you forget the filter."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "total_steps = EPOCHS * (len(train_loader.dataset) // BATCH_SIZE) + EPOCHS\n",
    "cosine_decay_scheduler = optax.cosine_decay_schedule(\n",
    "    LR, decay_steps=total_steps, alpha=0.95\n",
    ")\n",
    "optimizer = optax.adam(learning_rate=cosine_decay_scheduler)\n",
    "opt_state = optimizer.init(\n",
    "    eqx.filter(model, eqx.is_array)\n",
    ")  # Wrap in a fileter to avoid passing non-JAX types"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# The Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "loss = 0\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    for step, (x, y) in enumerate(train_loader):\n",
    "        key = jrandom.PRNGKey(epoch + x.shape[0] * step)\n",
    "        keys = jrandom.split(key, x.shape[0])\n",
    "        loss_value, model, opt_state = make_step(\n",
    "            model, jnp.asarray(x), jnp.asarray(y), keys, optimizer, opt_state\n",
    "        )\n",
    "        loss = 0.9 * loss + 0.1 * loss_value.item()\n",
    "\n",
    "    model = eqx.tree_inference(model, True)  # Analogous to model.eval()\n",
    "    train_acc = accuracy(model, train_loader)\n",
    "    test_acc = accuracy(model, val_loader)\n",
    "    model = eqx.tree_inference(model, False)  # Back to training mode\n",
    "\n",
    "    print(\n",
    "        f\"Epoch={epoch}, loss={loss:.4f}, tr.acc={train_acc.item():.4f}, te.acc={test_acc.item():.4f}\"\n",
    "    )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Verify Weights\n",
    "\n",
    "The last bit is to verify that weights of `model.features` is unchanged and only `model.classifier` is updated."
   ],
   "metadata": {
    "id": "YjSyCivdaqmE",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "base_model = eqx.tree_inference(eqv.models.vgg11(pretrained=True), True)\n",
    "assert eqx.tree_equal(base_model.features, model.features)"
   ],
   "metadata": {
    "id": "E93LbpK1HAjq",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 50,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "*That's all Folks*"
   ],
   "metadata": {
    "id": "4fJqBgD4bPKN",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ]
}