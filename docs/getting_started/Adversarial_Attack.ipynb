{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Adversarial Attack.ipynb",
   "provenance": [],
   "collapsed_sections": [],
   "authorship_tag": "ABX9TyMukl8DjS5ew5UJQfufBE/c",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/paganpasta/eqxvision/blob/dev/docs/getting_started/Adversarial_Attack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Adversarial Attack (FGSM)\n",
    "\n",
    "We will be making our way through generating an adversarial example using the Fast-Gradient Sign Method (FGSM). \n",
    "\n",
    "Based on Torchvision [tutorial](https://pytorch.org/tutorials/beginner/fgsm_tutorial.html), check it out for a more in-depth understanding of adversarial examples.\n",
    "\n",
    "\n",
    "The flow of this tutorial will be as:\n",
    "\n",
    "- Prepare input image\n",
    "- Initialise a model\n",
    "- Compute FGSM\n",
    "- Visualize the results\n",
    "\n",
    "---\n"
   ],
   "metadata": {
    "id": "8o25GNcDr24X"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Installing Dependencies"
   ],
   "metadata": {
    "id": "4BGFPhy-1h9D"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "CMsheGhyLTZv",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3f1b7e89-c199-4513-e1f6-77037a870f4a"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\u001b[?25l\r\u001b[K     |█████▋                          | 10 kB 22.9 MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 20 kB 16.0 MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 30 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████████▌         | 40 kB 4.7 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 51 kB 4.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 58 kB 2.6 MB/s \n",
      "\u001b[K     |████████████████████████████████| 145 kB 9.5 MB/s \n",
      "\u001b[K     |████████████████████████████████| 68 kB 6.0 MB/s \n",
      "\u001b[K     |████████████████████████████████| 76 kB 5.0 MB/s \n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install eqxvision optax --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Required Imports"
   ],
   "metadata": {
    "id": "1H3GxZSP1lWN"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import equinox as eqx\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrandom\n",
    "import matplotlib.pyplot as plt\n",
    "import optax\n",
    "import requests\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "\n",
    "import eqxvision as eqv\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Preparing Image & Transforms"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Download\n",
    "response = requests.get(\n",
    "    \"https://cdn.britannica.com/80/150980-050-84B9202C/Giant-panda-cub-branch.jpg\"\n",
    ")\n",
    "img = Image.open(BytesIO(response.content))\n",
    "img = img.convert(\"RGB\")\n",
    "\n",
    "# Transform\n",
    "mean = (0.485, 0.456, 0.406)\n",
    "std_dev = (0.229, 0.224, 0.225)\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean, std_dev),\n",
    "    ]\n",
    ")\n",
    "img = jnp.asarray(transform(img).unsqueeze(0).numpy())\n",
    "\n",
    "\n",
    "def inv_transform(x):\n",
    "    means = jnp.asarray(mean).reshape(3, -1)\n",
    "    std_devs = jnp.asarray(std_dev).reshape(3, -1)\n",
    "    func = lambda x, m, s: s * x + m\n",
    "    x = jax.vmap(func)(x, means, std_devs)\n",
    "    return jnp.transpose(x, (1, 2, 0))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Prediction to Class Names"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Prediction to class name mapping\n",
    "cls_map_link = (\n",
    "    \"https://github.com/Waikato/wekaDeeplearning4j/blob/master/src/main/resources/\"\n",
    "    \"class-maps/IMAGENET.txt?raw=True\"\n",
    ")\n",
    "data = urlopen(cls_map_link).read().decode(\"utf-8\").split(\"\\n\")\n",
    "cls_map = {}\n",
    "for i, line in enumerate(data):\n",
    "    cls_map[i] = line.strip().split(\",\")[0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialising Model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = eqv.models.resnet50(pretrained=True)\n",
    "model = eqx.tree_inference(model, True)\n",
    "key = jrandom.split(jrandom.PRNGKey(0), 1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Loss Computation\n",
    "\n",
    "Keep an eye out for the change in position of `x` and `model`. We aim to compute the gradient w.r.t the input `x` instead of the model parameters. \n",
    "Equinox by default computes the gradient w.r.t the `first argument`. Read more on this [here](https://docs.kidger.site/equinox/api/filtering/filtered-transformations/#equinox.filter_grad)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "@eqx.filter_value_and_grad\n",
    "def compute_loss(x, model, y, keys):\n",
    "    logits = jax.vmap(model, axis_name=(\"batch\"))(x, key=keys)\n",
    "    one_hot_actual = jax.nn.one_hot(y, num_classes=1000)\n",
    "    return optax.softmax_cross_entropy(logits, one_hot_actual).mean()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### FGSM Computation\n",
    "\n",
    "For an image $x$ and model $f$, the adversarial example is:\n",
    "\n",
    "$x_{adv} = x + ϵ \\times sign(\\nabla_x f(x))$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output = jax.vmap(model, axis_name=\"batch\")(img, key=key)\n",
    "pred_cls = jnp.argmax(output, axis=1).item()\n",
    "print(f\"Original Category: {cls_map[pred_cls]}\")\n",
    "\n",
    "loss, grads = compute_loss(img, model, pred_cls, key)\n",
    "del_x = 0.1 * jnp.sign(grads)\n",
    "adv_img = img + del_x\n",
    "\n",
    "output = jax.vmap(model, axis_name=\"batch\")(adv_img, key=key)\n",
    "pred_adv = jnp.argmax(output, axis=1).item()\n",
    "print(f\"Adversarial Category: {cls_map[pred_adv]}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Visualisation"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f, axs = plt.subplots(1, 3, figsize=(14, 14))\n",
    "axs[0].imshow(inv_transform(img[0]))\n",
    "axs[1].matshow(jnp.transpose(del_x[0], (1, 2, 0)))\n",
    "axs[2].imshow(inv_transform(adv_img[0]))\n",
    "axs[0].set_title(f\"{cls_map[pred_cls]}\")\n",
    "axs[1].set_title(\"grad. image\")\n",
    "axs[2].set_title(f\"{cls_map[pred_adv]}\")\n",
    "axs[0].axis(\"off\")\n",
    "axs[1].axis(\"off\")\n",
    "axs[2].axis(\"off\")\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "*That's all Folks*"
   ],
   "metadata": {
    "id": "5M6_3HEH3hrt"
   }
  }
 ]
}